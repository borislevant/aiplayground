{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "909dfed2",
   "metadata": {},
   "source": [
    "# PPO Implementation for the 15 Puzzle Game\n",
    "\n",
    "This notebook implements a Proximal Policy Optimization (PPO) agent to solve the 15 Puzzle Game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8997a0",
   "metadata": {},
   "source": [
    "## Define the 15 Puzzle Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FifteenPuzzleEnv:\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.arange(self.size * self.size)\n",
    "        np.random.shuffle(self.state)\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state.copy()\n",
    "\n",
    "    def is_done(self):\n",
    "        return np.array_equal(self.state, np.arange(self.size * self.size))\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        empty_pos = np.where(self.state == 0)[0][0]\n",
    "        x, y = divmod(empty_pos, self.size)\n",
    "        actions = []\n",
    "        if x > 0: actions.append('up')\n",
    "        if x < self.size - 1: actions.append('down')\n",
    "        if y > 0: actions.append('left')\n",
    "        if y < self.size - 1: actions.append('right')\n",
    "        return actions\n",
    "\n",
    "    def step(self, action):\n",
    "        empty_pos = np.where(self.state == 0)[0][0]\n",
    "        x, y = divmod(empty_pos, self.size)\n",
    "        new_pos = empty_pos\n",
    "        if action == 'up': new_pos -= self.size\n",
    "        elif action == 'down': new_pos += self.size\n",
    "        elif action == 'left': new_pos -= 1\n",
    "        elif action == 'right': new_pos += 1\n",
    "        self.state[empty_pos], self.state[new_pos] = self.state[new_pos], self.state[empty_pos]\n",
    "        reward = 1 if self.is_done() else -0.1\n",
    "        done = self.is_done()\n",
    "        return self.get_state(), reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e02899",
   "metadata": {},
   "source": [
    "## Define the PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(128, action_dim)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.policy_head(x), self.value_head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018750ae",
   "metadata": {},
   "source": [
    "## Training Loop (To Be Completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You would implement the PPO training loop here,\n",
    "# including rollout collection, advantage estimation,\n",
    "# and policy/value network updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875bfc3",
   "metadata": {},
   "source": [
    "## PPO Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42921c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, dones, values, gamma=0.99):\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "        if d:\n",
    "            G = 0\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return torch.tensor(returns, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfeb8b",
   "metadata": {},
   "source": [
    "## PPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FifteenPuzzleEnv()\n",
    "action_space = ['up', 'down', 'left', 'right']\n",
    "input_dim = env.size * env.size\n",
    "action_dim = len(action_space)\n",
    "model = ActorCritic(input_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "def select_action(state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    logits, value = model(state_tensor)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    dist = Categorical(probs)\n",
    "    action = dist.sample()\n",
    "    return action.item(), dist.log_prob(action), value\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "gamma = 0.99\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    actions_taken = []\n",
    "    states = []\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        states.append(state)\n",
    "        available_actions = env.get_possible_actions()\n",
    "        action_mask = [1 if a in available_actions else 0 for a in action_space]\n",
    "        logits, value = model(torch.tensor(state, dtype=torch.float32))\n",
    "        masked_logits = logits + torch.tensor(action_mask, dtype=torch.float32).log()\n",
    "        dist = Categorical(logits=masked_logits)\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done = env.step(action_space[action.item()])\n",
    "\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        actions_taken.append(action.item())\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    returns = compute_returns(rewards, dones, values, gamma)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantage = returns - values\n",
    "\n",
    "    policy_loss = -(log_probs * advantage.detach()).mean()\n",
    "    value_loss = advantage.pow(2).mean()\n",
    "    loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Total Reward: {total_reward:.2f}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec0e3d",
   "metadata": {},
   "source": [
    "## Complete PPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, action_space):\n",
    "        self.model = ActorCritic(input_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=3e-4)\n",
    "        self.action_space = action_space\n",
    "        self.eps_clip = 0.2\n",
    "        self.gamma = 0.99\n",
    "        self.entropy_coef = 0.01\n",
    "        self.value_coef = 0.5\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        logits, value = self.model(state)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), dist.entropy(), value\n",
    "\n",
    "    def compute_returns(self, rewards, dones, values, next_value):\n",
    "        returns = []\n",
    "        R = next_value\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            R = rewards[step] + self.gamma * R * (1 - dones[step])\n",
    "            returns.insert(0, R)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        states = torch.stack(trajectories['states'])\n",
    "        actions = torch.tensor(trajectories['actions'])\n",
    "        old_log_probs = torch.stack(trajectories['log_probs'])\n",
    "        returns = torch.tensor(trajectories['returns'])\n",
    "        values = torch.stack(trajectories['values']).squeeze()\n",
    "\n",
    "        advantages = returns - values.detach()\n",
    "\n",
    "        for _ in range(4):  # Multiple epochs\n",
    "            logits, new_values = self.model(states)\n",
    "            new_probs = torch.softmax(logits, dim=-1)\n",
    "            dist = Categorical(new_probs)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = (returns - new_values.squeeze()).pow(2).mean()\n",
    "            loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "# Train PPO on 15 Puzzle\n",
    "env = FifteenPuzzleEnv()\n",
    "agent = PPOAgent(input_dim=16, action_dim=4, action_space=['up', 'down', 'left', 'right'])\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    trajectories = {'states': [], 'actions': [], 'log_probs': [], 'rewards': [], 'dones': [], 'values': [], 'returns': []}\n",
    "\n",
    "    total_reward = 0\n",
    "    for t in range(200):\n",
    "        possible_actions = env.get_possible_actions()\n",
    "        action_map = ['up', 'down', 'left', 'right']\n",
    "        action_mask = [1 if a in possible_actions else 0 for a in action_map]\n",
    "\n",
    "        action, log_prob, entropy, value = agent.select_action(state)\n",
    "        while not action_mask[action]:\n",
    "            action, log_prob, entropy, value = agent.select_action(state)\n",
    "\n",
    "        next_state, reward, done = env.step(action_map[action])\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "\n",
    "        trajectories['states'].append(state)\n",
    "        trajectories['actions'].append(action)\n",
    "        trajectories['log_probs'].append(log_prob)\n",
    "        trajectories['rewards'].append(reward)\n",
    "        trajectories['dones'].append(done)\n",
    "        trajectories['values'].append(value)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    next_value = agent.model(state.unsqueeze(0))[1].detach().item()\n",
    "    trajectories['returns'] = agent.compute_returns(trajectories['rewards'], trajectories['dones'], trajectories['values'], next_value)\n",
    "\n",
    "    agent.update(trajectories)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6e792",
   "metadata": {},
   "source": [
    "## Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbc709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the trained agent\n",
    "def evaluate_agent(agent, episodes=10):\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state)\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        for t in range(200):\n",
    "            with torch.no_grad():\n",
    "                logits, _ = agent.model(state.unsqueeze(0))\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = Categorical(probs)\n",
    "                action = dist.sample().item()\n",
    "\n",
    "            action_map = ['up', 'down', 'left', 'right']\n",
    "            possible_actions = env.get_possible_actions()\n",
    "            action_mask = [1 if a in possible_actions else 0 for a in action_map]\n",
    "\n",
    "            while not action_mask[action]:\n",
    "                action = dist.sample().item()\n",
    "\n",
    "            next_state, reward, done = env.step(action_map[action])\n",
    "            state = torch.FloatTensor(next_state)\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "        steps.append(step_count)\n",
    "        print(f\"Evaluation Episode {ep+1}: Reward = {total_reward}, Steps = {step_count}\")\n",
    "    return rewards, steps\n",
    "\n",
    "# Run evaluation\n",
    "rewards, steps = evaluate_agent(agent, episodes=20)\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Rewards over Evaluation Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps)\n",
    "plt.title(\"Steps to Solve Puzzle per Evaluation Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
