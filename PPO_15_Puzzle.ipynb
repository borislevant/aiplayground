{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "909dfed2",
   "metadata": {},
   "source": [
    "# PPO Implementation for the 15 Puzzle Game\n",
    "\n",
    "This notebook implements a Proximal Policy Optimization (PPO) agent to solve the 15 Puzzle Game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8997a0",
   "metadata": {},
   "source": [
    "## Define the 15 Puzzle Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FifteenPuzzleEnv:\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.arange(self.size * self.size)\n",
    "        np.random.shuffle(self.state)\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state.copy()\n",
    "\n",
    "    def is_done(self):\n",
    "        return np.array_equal(self.state, np.arange(self.size * self.size))\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        empty_pos = np.where(self.state == 0)[0][0]\n",
    "        x, y = divmod(empty_pos, self.size)\n",
    "        actions = []\n",
    "        if x > 0: actions.append('up')\n",
    "        if x < self.size - 1: actions.append('down')\n",
    "        if y > 0: actions.append('left')\n",
    "        if y < self.size - 1: actions.append('right')\n",
    "        return actions\n",
    "\n",
    "    def step(self, action):\n",
    "        empty_pos = np.where(self.state == 0)[0][0]\n",
    "        x, y = divmod(empty_pos, self.size)\n",
    "        new_pos = empty_pos\n",
    "        if action == 'up': new_pos -= self.size\n",
    "        elif action == 'down': new_pos += self.size\n",
    "        elif action == 'left': new_pos -= 1\n",
    "        elif action == 'right': new_pos += 1\n",
    "        self.state[empty_pos], self.state[new_pos] = self.state[new_pos], self.state[empty_pos]\n",
    "        reward = 1 if self.is_done() else -0.1\n",
    "        done = self.is_done()\n",
    "        return self.get_state(), reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e02899",
   "metadata": {},
   "source": [
    "## Define the PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(128, action_dim)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.policy_head(x), self.value_head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018750ae",
   "metadata": {},
   "source": [
    "## Training Loop (To Be Completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You would implement the PPO training loop here,\n",
    "# including rollout collection, advantage estimation,\n",
    "# and policy/value network updates."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
